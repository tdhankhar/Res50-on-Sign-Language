{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import os\n",
    "from pathlib import Path\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_sign():\n",
    "    train_dataset = h5py.File('signs/train_signs.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('signs/test_signs.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(y, C):\n",
    "    y = np.eye(C)[y.reshape(-1)].T\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rock Paper Scissors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_rps():\n",
    "    labels={\n",
    "        'rock':0,\n",
    "        'paper':1,\n",
    "        'scissors':2,\n",
    "    }\n",
    "    p = Path(\"/home/tarun/Desktop/Projects Completed/ResNet/rps\")\n",
    "    train_set_x_orig=[]\n",
    "    train_set_y_orig=[]\n",
    "    direc = p.glob('*')\n",
    "\n",
    "    for d in direc:\n",
    "        fname=str(d)\n",
    "        label=fname.split(\"/\")[-1]\n",
    "        if label != '.DS_Store':\n",
    "            for j in d.glob('*'):\n",
    "                img=image.load_img(j,target_size=(64,64))\n",
    "                image_array=image.img_to_array(img)\n",
    "                train_set_x_orig.append(np.array(image_array))\n",
    "                train_set_y_orig.append(labels[label])\n",
    "    \n",
    "    train_set_x_orig=np.array(train_set_x_orig)\n",
    "    train_set_y_orig=np.array(train_set_y_orig)\n",
    "\n",
    "    test_set_x_orig=[]\n",
    "    test_set_y_orig=[]\n",
    "    p = Path(\"/home/tarun/Desktop/Projects Completed/ResNet/rps-test-set\")\n",
    "    direc = p.glob('*')\n",
    "    for d in direc:\n",
    "        fname=str(d)\n",
    "        label=fname.split(\"/\")[-1]\n",
    "        if(label != '.DS_Store'):\n",
    "            for j in d.glob('*'):\n",
    "                img=image.load_img(j,target_size=(64,64))\n",
    "                img=image.img_to_array(img)\n",
    "                test_set_x_orig.append(img)\n",
    "                test_set_y_orig.append(labels[label])\n",
    "\n",
    "    test_set_x_orig=np.array(test_set_x_orig)\n",
    "    test_set_y_orig=np.array(test_set_y_orig)\n",
    "    \n",
    "    classes = [0, 1, 2]\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 2520\n",
      "number of test examples = 372\n",
      "number of classes = 3\n",
      "X_train shape: (2520, 64, 64, 3)\n",
      "y_train shape: (2520, 3)\n",
      "X_test shape: (372, 64, 64, 3)\n",
      "y_test shape: (372, 3)\n",
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "X_train_orig, y_train_orig, X_test_orig, y_test_orig, classes = load_dataset_rps()\n",
    "\n",
    "# Normalize image vectors\n",
    "X_train = X_train_orig/255.\n",
    "X_test = X_test_orig/255.\n",
    "\n",
    "# Convert training and test labels to one hot matrices\n",
    "y_train = convert_to_one_hot(y_train_orig, len(classes)).T\n",
    "y_test = convert_to_one_hot(y_test_orig, len(classes)).T\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"number of classes = \" + str(len(classes)))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"y_train shape: \" + str(y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"y_test shape: \" + str(y_test.shape))\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_h, n_w, n_c, n_y):\n",
    "    '''\n",
    "    Arguments:\n",
    "    n_h -- height of the input image \n",
    "    n_w -- width of the input image\n",
    "    n_c -- channels of the input image\n",
    "    n_y -- number of classes\n",
    "    \n",
    "    Returns:\n",
    "    X -- placeholder for the data inputs of shape (None, n_h, n_w, n_c)\n",
    "    y -- placeholder for the output labels of shape (None, n_y)\n",
    "    is_training -- boolean placeholder for batch normalization\n",
    "    '''\n",
    "    X = tf.placeholder(tf.float32, shape = (None, n_h, n_w, n_c))\n",
    "    y = tf.placeholder(tf.float32, shape = (None, n_y))\n",
    "    is_training = tf.placeholder_with_default(False, (), 'is_training')\n",
    "    return X, y, is_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_padding(X, pad):\n",
    "    '''\n",
    "    Arguments:\n",
    "    X -- input dataset\n",
    "    pad -- zero padding\n",
    "    \n",
    "    Returns:\n",
    "    X_pad -- padded input dataset\n",
    "    '''\n",
    "    X_pad = tf.pad(X, tf.constant([[0, 0], [pad, pad], [pad, pad], [0, 0]]), \"CONSTANT\")\n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    '''\n",
    "    Returns:\n",
    "    parameters -- a dictionary containing tensors for weights\n",
    "    '''\n",
    "    parameters = {\n",
    "        # Stage 1\n",
    "        'b1' : {\n",
    "            'w1' : tf.get_variable('w0', shape = [7, 7, 3, 64], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        },\n",
    "        \n",
    "        # Stage 2\n",
    "        'b2' : {\n",
    "            'w1' : tf.get_variable('w1', shape = [1, 1, 64, 64], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w2' : tf.get_variable('w2', shape = [3, 3, 64, 64], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w3' : tf.get_variable('w3a', shape = [1, 1, 64, 256], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w4' : tf.get_variable('w3b', shape = [1, 1, 64, 256], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        },\n",
    "        'b3' : {\n",
    "            'w1' : tf.get_variable('w4', shape = [1, 1, 256, 64], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w2' : tf.get_variable('w5', shape = [3, 3, 64, 64], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w3' : tf.get_variable('w6', shape = [1, 1, 64, 256], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        },\n",
    "        'b4' : {\n",
    "            'w1' : tf.get_variable('w7', shape = [1, 1, 256, 64], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w2' : tf.get_variable('w8', shape = [3, 3, 64, 64], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w3' : tf.get_variable('w9', shape = [1, 1, 64, 256], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        },\n",
    "        \n",
    "        # Stage 3\n",
    "        'b5' : {\n",
    "            'w1' : tf.get_variable('w10', shape = [1, 1, 256, 128], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w2' : tf.get_variable('w11', shape = [3, 3, 128, 128], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w3' : tf.get_variable('w12a', shape = [1, 1, 128, 512], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w4' : tf.get_variable('w12b', shape = [1, 1, 256, 512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        },\n",
    "        'b6' : {\n",
    "            'w1' : tf.get_variable('w13', shape = [1, 1, 512, 128], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w2' : tf.get_variable('w14', shape = [3, 3, 128, 128], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w3' : tf.get_variable('w15', shape = [1, 1, 128, 512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        },\n",
    "        'b7' : {\n",
    "            'w1' : tf.get_variable('w16', shape = [1, 1, 512, 128], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w2' : tf.get_variable('w17', shape = [3, 3, 128, 128], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w3' : tf.get_variable('w18', shape = [1, 1, 128, 512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        },\n",
    "        'b8' : {\n",
    "            'w1' : tf.get_variable('w19', shape = [1, 1, 512, 128], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w2' : tf.get_variable('w20', shape = [3, 3, 128, 128], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w3' : tf.get_variable('w21', shape = [1, 1, 128, 512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        },\n",
    "        \n",
    "        # Stage 4\n",
    "        'b9' : {\n",
    "            'w1' : tf.get_variable('w22', shape = [1, 1, 512, 256], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w2' : tf.get_variable('w23', shape = [3, 3, 256, 256], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w3' : tf.get_variable('w24a', shape = [1, 1, 256, 1024], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w4' : tf.get_variable('w24b', shape = [1, 1, 512, 1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        },\n",
    "        'b10' : {\n",
    "            'w1' : tf.get_variable('w25', shape = [1, 1, 1024, 256], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w2' : tf.get_variable('w26', shape = [3, 3, 256, 256], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w3' : tf.get_variable('w27', shape = [1, 1, 256, 1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        },\n",
    "        'b11' : {\n",
    "            'w1' : tf.get_variable('w28', shape = [1, 1, 1024, 256], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w2' : tf.get_variable('w29', shape = [3, 3, 256, 256], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w3' : tf.get_variable('w30', shape = [1, 1, 256, 1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        },\n",
    "        'b12' : {\n",
    "            'w1' : tf.get_variable('w31', shape = [1, 1, 1024, 256], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w2' : tf.get_variable('w32', shape = [3, 3, 256, 256], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w3' : tf.get_variable('w33', shape = [1, 1, 256, 1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        },\n",
    "        'b13' : {\n",
    "            'w1' : tf.get_variable('w34', shape = [1, 1, 1024, 256], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w2' : tf.get_variable('w35', shape = [3, 3, 256, 256], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w3' : tf.get_variable('w36', shape = [1, 1, 256, 1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        },\n",
    "        'b14' : {\n",
    "            'w1' : tf.get_variable('w37', shape = [1, 1, 1024, 256], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w2' : tf.get_variable('w38', shape = [3, 3, 256, 256], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w3' : tf.get_variable('w39', shape = [1, 1, 256, 1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        },\n",
    "        \n",
    "        # Stage 5\n",
    "        'b15' : {\n",
    "            'w1' : tf.get_variable('w40', shape = [1, 1, 1024, 512], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w2' : tf.get_variable('w41', shape = [3, 3, 512, 512], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w3' : tf.get_variable('w42a', shape = [1, 1, 512, 2048], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w4' : tf.get_variable('w42b', shape = [1, 1, 1024, 2048], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        },\n",
    "        'b16' : {\n",
    "            'w1' : tf.get_variable('w43', shape = [1, 1, 2048, 512], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w2' : tf.get_variable('w44', shape = [3, 3, 512, 512], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w3' : tf.get_variable('w45', shape = [1, 1, 512, 2048], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        },\n",
    "        'b17' : {\n",
    "            'w1' : tf.get_variable('w46', shape = [1, 1, 2048, 512], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w2' : tf.get_variable('w47', shape = [3, 3, 512, 512], initializer = tf.contrib.layers.xavier_initializer()),\n",
    "            'w3' : tf.get_variable('w48', shape = [1, 1, 512, 2048], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        }\n",
    "    }\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(X, parameters, is_training):\n",
    "    '''\n",
    "    Arguments:\n",
    "    X -- input dataset\n",
    "    parameters -- python dictionary for weights\n",
    "    is_training -- boolean tensor for batch normalization\n",
    "    \n",
    "    Returns:\n",
    "    X -- output dataset\n",
    "    '''\n",
    "    X_shortcut = X\n",
    "    \n",
    "    X = tf.nn.conv2d(X, parameters['w1'], strides = [1,1,1,1], padding = 'VALID')\n",
    "    X = tf.layers.batch_normalization(X, axis = 3, training=is_training)\n",
    "    X = tf.nn.relu(X)\n",
    "    \n",
    "    X = tf.nn.conv2d(X, parameters['w2'], strides = [1,1,1,1], padding = 'SAME')\n",
    "    X = tf.layers.batch_normalization(X, axis = 3, training=is_training)\n",
    "    X = tf.nn.relu(X)\n",
    "    \n",
    "    X = tf.nn.conv2d(X, parameters['w3'], strides = [1,1,1,1], padding = 'VALID')\n",
    "    X = tf.layers.batch_normalization(X, axis = 3, training=is_training)\n",
    "    \n",
    "    X = tf.add(X_shortcut, X)\n",
    "    \n",
    "    X = tf.nn.relu(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(X, parameters, is_training, s = 2):\n",
    "    '''\n",
    "    Arguments:\n",
    "    X -- input dataset\n",
    "    parameters -- python dictionary for weights\n",
    "    is_training -- boolean tensor for batch normalization\n",
    "    s -- stride\n",
    "    \n",
    "    Returns:\n",
    "    X -- output dataset\n",
    "    '''\n",
    "    X_shortcut = X\n",
    "    \n",
    "    X = tf.nn.conv2d(X, parameters['w1'], strides = [1,s,s,1], padding = 'VALID')\n",
    "    X = tf.layers.batch_normalization(X, axis = 3, training=is_training)\n",
    "    X = tf.nn.relu(X)\n",
    "    \n",
    "    X = tf.nn.conv2d(X, parameters['w2'], strides = [1,1,1,1], padding = 'SAME')\n",
    "    X = tf.layers.batch_normalization(X, axis = 3, training=is_training)\n",
    "    X = tf.nn.relu(X)\n",
    "    \n",
    "    X = tf.nn.conv2d(X, parameters['w3'], strides = [1,1,1,1], padding = 'VALID')\n",
    "    X = tf.layers.batch_normalization(X, axis = 3, training=is_training)\n",
    "    \n",
    "    X_shortcut = tf.nn.conv2d(X_shortcut, parameters['w4'], strides = [1,s,s,1], padding = 'VALID')\n",
    "    X_shortcut = tf.layers.batch_normalization(X_shortcut, axis = 3, training=is_training)\n",
    "    \n",
    "    X = tf.add(X_shortcut, X)\n",
    "    \n",
    "    X = tf.nn.relu(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, is_training, n_y):\n",
    "    '''\n",
    "    Arguments:\n",
    "    X -- input dataset placeholder of shape (m, n_h, n_w, n_c)\n",
    "    parameters -- python dict of dicts for weights\n",
    "    is_training -- boolean value for batch normalization\n",
    "    n_y -- number of classes\n",
    "    \n",
    "    Returns:\n",
    "    Z -- output of the last linear unit\n",
    "    '''\n",
    "    # Stage 1\n",
    "    X = zero_padding(X, 3)\n",
    "    X = tf.nn.conv2d(X, parameters['b1']['w1'], strides = [1,2,2,1], padding = 'VALID')\n",
    "    X = tf.layers.batch_normalization(X, axis = 3, training=is_training)\n",
    "    X = tf.nn.relu(X)\n",
    "    X = tf.nn.max_pool(X, ksize = [1,3,3,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "    \n",
    "    # Stage 2\n",
    "    X = conv_block(X, parameters['b2'], is_training, s = 1)\n",
    "    X = identity_block(X, parameters['b3'], is_training)\n",
    "    X = identity_block(X, parameters['b4'], is_training)\n",
    "    \n",
    "    # Stage 3\n",
    "    X = conv_block(X, parameters['b5'], is_training, s = 2)\n",
    "    X = identity_block(X, parameters['b6'], is_training)\n",
    "    X = identity_block(X, parameters['b7'], is_training)\n",
    "    X = identity_block(X, parameters['b8'], is_training)\n",
    "    \n",
    "    #Stage 4\n",
    "    X = conv_block(X, parameters['b9'], is_training, s = 2)\n",
    "    X = identity_block(X, parameters['b10'], is_training)\n",
    "    X = identity_block(X, parameters['b11'], is_training)\n",
    "    X = identity_block(X, parameters['b12'], is_training)\n",
    "    X = identity_block(X, parameters['b13'], is_training)\n",
    "    X = identity_block(X, parameters['b14'], is_training)\n",
    "    \n",
    "    # Stage 5\n",
    "    X = conv_block(X, parameters['b15'], is_training, s = 2)\n",
    "    X = identity_block(X, parameters['b16'], is_training)\n",
    "    X = identity_block(X, parameters['b17'], is_training)\n",
    "    \n",
    "    X = tf.nn.avg_pool(X, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "    \n",
    "    F = tf.contrib.layers.flatten(X)\n",
    "    Z = tf.contrib.layers.fully_connected(F, n_y, activation_fn = None)\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z, y):\n",
    "    '''\n",
    "    Arguments:\n",
    "    Z -- output of the last linear unit\n",
    "    y -- target labels\n",
    "    \n",
    "    Returns:\n",
    "    cost -- softmax cross entropy cost\n",
    "    '''\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z, labels = y))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, y, mini_batch_size = 64, seed = 0):\n",
    "    '''\n",
    "    Arguments:\n",
    "    X -- input dataset\n",
    "    Y -- target labels\n",
    "    mini_batch_size -- size of mini batches\n",
    "    seed -- seed value for random functions\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of random mini batches\n",
    "    '''\n",
    "    m = X.shape[0] \n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation,:]\n",
    "    shuffled_y = y[permutation,:]\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_mini_batches = int(m/mini_batch_size)\n",
    "    for k in range(0, num_complete_mini_batches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n",
    "        mini_batch_y = shuffled_y[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_mini_batches * mini_batch_size : m,:]\n",
    "        mini_batch_y = shuffled_y[num_complete_mini_batches * mini_batch_size : m,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_train, n_h, n_w, n_c = X_train.shape\n",
    "m_train, n_y = y_train.shape\n",
    "\n",
    "m_test, n_h, n_w, n_c = X_test.shape\n",
    "m_test, n_y = y_test.shape\n",
    "\n",
    "px, py, is_training = create_placeholders(n_h, n_w, n_c, n_y)\n",
    "parameters = initialize_parameters()\n",
    "\n",
    "Z = forward_propagation(px, parameters, is_training, n_y)\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "cost = compute_cost(Z, py)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(cost)\n",
    "\n",
    "predictions = tf.argmax(Z,1)\n",
    "labels = tf.argmax(py,1)\n",
    "results = tf.equal(predictions, labels)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Cost => 0.6404927954247871 | Training Accuracy => 33.33333333333333 | Validation Accuracy 33.33333333333333\n",
      "2) Cost => 0.0780352591138394 | Training Accuracy => 33.33333333333333 | Validation Accuracy 33.33333333333333\n",
      "3) Cost => 0.045933807397204866 | Training Accuracy => 37.77777777777778 | Validation Accuracy 33.33333333333333\n",
      "4) Cost => 0.041731552358783106 | Training Accuracy => 88.96825396825396 | Validation Accuracy 71.7741935483871\n",
      "5) Cost => 0.013540389340949682 | Training Accuracy => 99.28571428571429 | Validation Accuracy 77.15053763440861\n",
      "6) Cost => 0.006960927563035022 | Training Accuracy => 99.76190476190476 | Validation Accuracy 79.83870967741935\n",
      "7) Cost => 0.0025776099766447328 | Training Accuracy => 99.96031746031746 | Validation Accuracy 88.70967741935483\n",
      "8) Cost => 0.0014780069967045147 | Training Accuracy => 99.72222222222223 | Validation Accuracy 70.96774193548387\n",
      "9) Cost => 0.00027312657179214884 | Training Accuracy => 100.0 | Validation Accuracy 75.0\n",
      "10) Cost => 0.008610295584193092 | Training Accuracy => 100.0 | Validation Accuracy 79.3010752688172\n",
      "11) Cost => 0.004500691276462751 | Training Accuracy => 98.21428571428571 | Validation Accuracy 65.05376344086021\n",
      "12) Cost => 0.01347818802111843 | Training Accuracy => 98.88888888888889 | Validation Accuracy 77.15053763440861\n",
      "13) Cost => 0.00047694933625618246 | Training Accuracy => 100.0 | Validation Accuracy 80.64516129032258\n",
      "14) Cost => 4.971560039953494e-05 | Training Accuracy => 100.0 | Validation Accuracy 79.83870967741935\n",
      "Training Done!!\n"
     ]
    }
   ],
   "source": [
    "mini_batch_size = 32\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    mini_batch_cost = 0\n",
    "    num_minibatches = int(m_train/mini_batch_size)\n",
    "    mini_batches = random_mini_batches(X_train, y_train, mini_batch_size, seed)\n",
    "    seed += 1\n",
    "    for mini_batch in mini_batches:\n",
    "        mini_batch_X, mini_batch_y = mini_batch\n",
    "        _, c = sess.run(fetches = [optimizer, cost], feed_dict = {px : mini_batch_X, py : mini_batch_y, is_training : True})\n",
    "        mini_batch_cost += c/num_minibatches\n",
    "\n",
    "    predictions_, labels_, results_ = sess.run([predictions, labels, results], feed_dict = {px : X_train, py : y_train})\n",
    "    train_score = results_.sum()/m_train\n",
    "    predictions_, labels_, results_ = sess.run([predictions, labels, results], feed_dict = {px : X_test, py : y_test})\n",
    "    val_score = results_.sum()/m_test\n",
    "    print(f\"{epoch+1}) Cost => {mini_batch_cost} | Training Accuracy => {train_score*100} | Validation Accuracy {val_score*100}\")\n",
    "    if mini_batch_cost < 0.0001:\n",
    "        print('Training Done!!')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
